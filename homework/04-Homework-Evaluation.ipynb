{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# ML Zoomcamp Homework 4: Evaluation Metrics for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "Load the Course Lead Scoring dataset and prepare it for analysis.\n",
    "\n",
    "Dataset: https://raw.githubusercontent.com/alexeygrigorev/datasets/master/course_lead_scoring.csv\n",
    "\n",
    "Target variable: `converted` (binary - whether client signed up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1462, 9)\n",
      "\n",
      "Column names:\n",
      "['lead_source', 'industry', 'number_of_courses_viewed', 'annual_income', 'employment_status', 'location', 'interaction_count', 'lead_score', 'converted']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lead_source</th>\n",
       "      <th>industry</th>\n",
       "      <th>number_of_courses_viewed</th>\n",
       "      <th>annual_income</th>\n",
       "      <th>employment_status</th>\n",
       "      <th>location</th>\n",
       "      <th>interaction_count</th>\n",
       "      <th>lead_score</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>paid_ads</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>79450.0</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>south_america</td>\n",
       "      <td>4</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>social_media</td>\n",
       "      <td>retail</td>\n",
       "      <td>1</td>\n",
       "      <td>46992.0</td>\n",
       "      <td>employed</td>\n",
       "      <td>south_america</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>events</td>\n",
       "      <td>healthcare</td>\n",
       "      <td>5</td>\n",
       "      <td>78796.0</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>australia</td>\n",
       "      <td>3</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>paid_ads</td>\n",
       "      <td>retail</td>\n",
       "      <td>2</td>\n",
       "      <td>83843.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>australia</td>\n",
       "      <td>1</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>referral</td>\n",
       "      <td>education</td>\n",
       "      <td>3</td>\n",
       "      <td>85012.0</td>\n",
       "      <td>self_employed</td>\n",
       "      <td>europe</td>\n",
       "      <td>3</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lead_source    industry  number_of_courses_viewed  annual_income  \\\n",
       "0      paid_ads         NaN                         1        79450.0   \n",
       "1  social_media      retail                         1        46992.0   \n",
       "2        events  healthcare                         5        78796.0   \n",
       "3      paid_ads      retail                         2        83843.0   \n",
       "4      referral   education                         3        85012.0   \n",
       "\n",
       "  employment_status       location  interaction_count  lead_score  converted  \n",
       "0        unemployed  south_america                  4        0.94          1  \n",
       "1          employed  south_america                  1        0.80          0  \n",
       "2        unemployed      australia                  3        0.69          1  \n",
       "3               NaN      australia                  1        0.87          0  \n",
       "4     self_employed         europe                  3        0.62          1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('data/course_lead_scoring.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "### Handle Missing Values\n",
    "\n",
    "- Replace missing categorical values with 'NA'\n",
    "- Replace missing numerical values with 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "lead_source                 128\n",
      "industry                    134\n",
      "number_of_courses_viewed      0\n",
      "annual_income               181\n",
      "employment_status           100\n",
      "location                     63\n",
      "interaction_count             0\n",
      "lead_score                    0\n",
      "converted                     0\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "lead_source                  object\n",
      "industry                     object\n",
      "number_of_courses_viewed      int64\n",
      "annual_income               float64\n",
      "employment_status            object\n",
      "location                     object\n",
      "interaction_count             int64\n",
      "lead_score                  float64\n",
      "converted                     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['lead_source', 'industry', 'employment_status', 'location']\n",
      "Numerical columns: ['number_of_courses_viewed', 'annual_income', 'interaction_count', 'lead_score']\n",
      "\n",
      "Missing values after filling:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical and numerical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Remove target from numerical columns if present\n",
    "if 'converted' in numerical_cols:\n",
    "    numerical_cols.remove('converted')\n",
    "\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "print(f\"Numerical columns: {numerical_cols}\")\n",
    "\n",
    "# Fill missing values\n",
    "for col in categorical_cols:\n",
    "    df[col] = df[col].fillna('NA')\n",
    "\n",
    "for col in numerical_cols:\n",
    "    df[col] = df[col].fillna(0.0)\n",
    "\n",
    "print(f\"\\nMissing values after filling:\")\n",
    "print(df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "### Split the data into train/validation/test sets (60%/20%/20%)\n",
    "\n",
    "Use `train_test_split` with `random_state=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 876 (59.9%)\n",
      "Validation set size: 293 (20.0%)\n",
      "Test set size: 293 (20.0%)\n",
      "\n",
      "Target distribution in train:\n",
      "converted\n",
      "1    0.621005\n",
      "0    0.378995\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# First split: 80% train+val, 20% test\n",
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)\n",
    "\n",
    "# Second split: 60% train, 20% val (from the 80%)\n",
    "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=1)\n",
    "\n",
    "print(f\"Train set size: {len(df_train)} ({len(df_train)/len(df)*100:.1f}%)\")\n",
    "print(f\"Validation set size: {len(df_val)} ({len(df_val)/len(df)*100:.1f}%)\")\n",
    "print(f\"Test set size: {len(df_test)} ({len(df_test)/len(df)*100:.1f}%)\")\n",
    "print(f\"\\nTarget distribution in train:\")\n",
    "print(df_train['converted'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "y_train = df_train['converted'].values\n",
    "y_val = df_val['converted'].values\n",
    "y_test = df_test['converted'].values\n",
    "\n",
    "# For full train (train + val) - needed for Question 5\n",
    "y_full_train = df_full_train['converted'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Question 1: ROC AUC Feature Importance\n",
    "\n",
    "For each numerical variable, use it as a score (prediction) and compute the AUC with the target variable.\n",
    "\n",
    "If AUC < 0.5, invert the variable by putting \"-\" in front.\n",
    "\n",
    "Which numerical variable has the highest AUC among:\n",
    "- `lead_score`\n",
    "- `number_of_courses_viewed`\n",
    "- `interaction_count`\n",
    "- `annual_income`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lead_score: AUC = 0.614\n",
      "number_of_courses_viewed: AUC = 0.764\n",
      "interaction_count: AUC = 0.738\n",
      "annual_income: AUC = 0.552\n",
      "\n",
      "✓ Answer: number_of_courses_viewed with AUC = 0.764\n"
     ]
    }
   ],
   "source": [
    "# Numerical variables to test\n",
    "numerical_vars = ['lead_score', 'number_of_courses_viewed', 'interaction_count', 'annual_income']\n",
    "\n",
    "auc_scores = {}\n",
    "\n",
    "for var in numerical_vars:\n",
    "    # Use the variable as prediction (score)\n",
    "    scores = df_train[var].values\n",
    "    \n",
    "    # Calculate AUC\n",
    "    auc = roc_auc_score(y_train, scores)\n",
    "    \n",
    "    # If AUC < 0.5, invert the variable\n",
    "    if auc < 0.5:\n",
    "        auc = roc_auc_score(y_train, -scores)\n",
    "        print(f\"{var}: AUC = {auc:.3f} (inverted)\")\n",
    "    else:\n",
    "        print(f\"{var}: AUC = {auc:.3f}\")\n",
    "    \n",
    "    auc_scores[var] = auc\n",
    "\n",
    "# Find the variable with highest AUC\n",
    "best_var = max(auc_scores, key=auc_scores.get)\n",
    "print(f\"\\n✓ Answer: {best_var} with AUC = {auc_scores[best_var]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Question 2: Training the Model\n",
    "\n",
    "Apply one-hot encoding using `DictVectorizer` and train a logistic regression model:\n",
    "\n",
    "```python\n",
    "LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "```\n",
    "\n",
    "What's the AUC of this model on the validation dataset? (round to 3 digits)\n",
    "\n",
    "Options: 0.32 / 0.52 / 0.72 / 0.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (876, 31)\n",
      "Validation set shape: (293, 31)\n",
      "Number of features after one-hot encoding: 31\n"
     ]
    }
   ],
   "source": [
    "# Prepare features (exclude target 'converted')\n",
    "features = [col for col in df_train.columns if col != 'converted']\n",
    "\n",
    "# Convert to dictionaries for DictVectorizer\n",
    "train_dicts = df_train[features].to_dict(orient='records')\n",
    "val_dicts = df_val[features].to_dict(orient='records')\n",
    "\n",
    "# Apply one-hot encoding\n",
    "dv = DictVectorizer(sparse=False)\n",
    "X_train = dv.fit_transform(train_dicts)\n",
    "X_val = dv.transform(val_dicts)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Number of features after one-hot encoding: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Answer: AUC on validation dataset = 0.817\n"
     ]
    }
   ],
   "source": [
    "# Train logistic regression model\n",
    "model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get prediction probabilities for validation set\n",
    "y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Calculate AUC\n",
    "auc_val = roc_auc_score(y_val, y_pred_proba)\n",
    "print(f\"✓ Answer: AUC on validation dataset = {round(auc_val, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Question 3: Precision and Recall\n",
    "\n",
    "Compute precision and recall for all thresholds from 0.0 to 1.0 with step 0.01.\n",
    "\n",
    "At which threshold do precision and recall curves intersect?\n",
    "\n",
    "Options: 0.145 / 0.345 / 0.545 / 0.745"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create thresholds from 0.0 to 1.0 with step 0.01\n",
    "thresholds = np.arange(0.0, 1.01, 0.01)\n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Convert probabilities to binary predictions using threshold\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    # Handle edge cases where there are no positive predictions\n",
    "    if y_pred.sum() == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = precision_score(y_val, y_pred, zero_division=0)\n",
    "    \n",
    "    recall = recall_score(y_val, y_pred, zero_division=0)\n",
    "    \n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "\n",
    "# Convert to numpy arrays for easier manipulation\n",
    "precisions = np.array(precisions)\n",
    "recalls = np.array(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot precision and recall curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, precisions, label='Precision', linewidth=2)\n",
    "plt.plot(thresholds, recalls, label='Recall', linewidth=2)\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision and Recall vs Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the intersection point (where precision and recall are closest)\n",
    "differences = np.abs(precisions - recalls)\n",
    "intersection_idx = np.argmin(differences)\n",
    "intersection_threshold = thresholds[intersection_idx]\n",
    "\n",
    "print(f\"Threshold where precision and recall intersect: {intersection_threshold:.3f}\")\n",
    "print(f\"Precision at intersection: {precisions[intersection_idx]:.3f}\")\n",
    "print(f\"Recall at intersection: {recalls[intersection_idx]:.3f}\")\n",
    "print(f\"Difference: {differences[intersection_idx]:.6f}\")\n",
    "print(f\"\\n✓ Answer: {intersection_threshold:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Question 4: F1 Score\n",
    "\n",
    "Precision and recall are conflicting. They are often combined into the F1 score:\n",
    "\n",
    "$$F_1 = 2 \\cdot \\frac{P \\cdot R}{P + R}$$\n",
    "\n",
    "Compute F1 for all thresholds from 0.0 to 1.0 with increment 0.01.\n",
    "\n",
    "At which threshold is F1 maximal?\n",
    "\n",
    "Options: 0.14 / 0.34 / 0.54 / 0.74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 scores for all thresholds\n",
    "f1_scores = []\n",
    "\n",
    "for i, threshold in enumerate(thresholds):\n",
    "    p = precisions[i]\n",
    "    r = recalls[i]\n",
    "    \n",
    "    # Calculate F1 score: F1 = 2 * (P * R) / (P + R)\n",
    "    if p + r == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2 * (p * r) / (p + r)\n",
    "    \n",
    "    f1_scores.append(f1)\n",
    "\n",
    "f1_scores = np.array(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot F1 score\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, f1_scores, linewidth=2, color='green')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score vs Threshold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the threshold with maximum F1 score\n",
    "max_f1_idx = np.argmax(f1_scores)\n",
    "max_f1_threshold = thresholds[max_f1_idx]\n",
    "max_f1_score = f1_scores[max_f1_idx]\n",
    "\n",
    "print(f\"Threshold with maximum F1 score: {max_f1_threshold:.2f}\")\n",
    "print(f\"Maximum F1 score: {max_f1_score:.3f}\")\n",
    "print(f\"Precision at max F1: {precisions[max_f1_idx]:.3f}\")\n",
    "print(f\"Recall at max F1: {recalls[max_f1_idx]:.3f}\")\n",
    "print(f\"\\n✓ Answer: {max_f1_threshold:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Question 5: 5-Fold Cross-Validation\n",
    "\n",
    "Use the `KFold` class from Scikit-Learn to evaluate the model on 5 different folds:\n",
    "\n",
    "```python\n",
    "KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "```\n",
    "\n",
    "- Iterate over different folds of `df_full_train`\n",
    "- Train the model: `LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)`\n",
    "- Use AUC to evaluate the model on validation\n",
    "\n",
    "What is the standard deviation of the AUC scores across different folds?\n",
    "\n",
    "Options: 0.0001 / 0.006 / 0.06 / 0.36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize KFold\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "auc_scores_cv = []\n",
    "\n",
    "# Iterate over folds\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(df_full_train), 1):\n",
    "    # Split data\n",
    "    df_fold_train = df_full_train.iloc[train_idx]\n",
    "    df_fold_val = df_full_train.iloc[val_idx]\n",
    "    \n",
    "    y_fold_train = df_fold_train['converted'].values\n",
    "    y_fold_val = df_fold_val['converted'].values\n",
    "    \n",
    "    # Prepare features\n",
    "    features_fold = [col for col in df_fold_train.columns if col != 'converted']\n",
    "    fold_train_dicts = df_fold_train[features_fold].to_dict(orient='records')\n",
    "    fold_val_dicts = df_fold_val[features_fold].to_dict(orient='records')\n",
    "    \n",
    "    # One-hot encoding\n",
    "    dv_fold = DictVectorizer(sparse=False)\n",
    "    X_fold_train = dv_fold.fit_transform(fold_train_dicts)\n",
    "    X_fold_val = dv_fold.transform(fold_val_dicts)\n",
    "    \n",
    "    # Train model\n",
    "    model_fold = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "    model_fold.fit(X_fold_train, y_fold_train)\n",
    "    \n",
    "    # Predict and calculate AUC\n",
    "    y_fold_pred_proba = model_fold.predict_proba(X_fold_val)[:, 1]\n",
    "    auc_fold = roc_auc_score(y_fold_val, y_fold_pred_proba)\n",
    "    \n",
    "    auc_scores_cv.append(auc_fold)\n",
    "    print(f\"Fold {fold}: AUC = {auc_fold:.3f}\")\n",
    "\n",
    "# Calculate statistics\n",
    "mean_auc = np.mean(auc_scores_cv)\n",
    "std_auc = np.std(auc_scores_cv)\n",
    "\n",
    "print(f\"\\nMean AUC: {mean_auc:.3f}\")\n",
    "print(f\"Standard deviation of AUC: {std_auc:.3f}\")\n",
    "print(f\"\\n✓ Answer: {std_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Question 6: Hyperparameter Tuning\n",
    "\n",
    "Use 5-Fold cross-validation to find the best parameter `C`.\n",
    "\n",
    "- Test `C` values: `[0.000001, 0.001, 1]`\n",
    "- Use `KFold(n_splits=5, shuffle=True, random_state=1)`\n",
    "- Model: `LogisticRegression(solver='liblinear', C=C, max_iter=1000)`\n",
    "- Compute the mean score and std (round to 3 decimal digits)\n",
    "\n",
    "Which `C` leads to the best mean score?\n",
    "\n",
    "If you have ties, select the score with the lowest std. If you still have ties, select the smallest `C`.\n",
    "\n",
    "Options: 0.000001 / 0.001 / 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C values to test\n",
    "c_values = [0.000001, 0.001, 1]\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "for c in c_values:\n",
    "    print(f\"\\nTesting C = {c}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    auc_scores_c = []\n",
    "    \n",
    "    # Initialize KFold with same parameters\n",
    "    kfold_c = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    \n",
    "    # Iterate over folds\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold_c.split(df_full_train), 1):\n",
    "        # Split data\n",
    "        df_fold_train = df_full_train.iloc[train_idx]\n",
    "        df_fold_val = df_full_train.iloc[val_idx]\n",
    "        \n",
    "        y_fold_train = df_fold_train['converted'].values\n",
    "        y_fold_val = df_fold_val['converted'].values\n",
    "        \n",
    "        # Prepare features\n",
    "        features_fold = [col for col in df_fold_train.columns if col != 'converted']\n",
    "        fold_train_dicts = df_fold_train[features_fold].to_dict(orient='records')\n",
    "        fold_val_dicts = df_fold_val[features_fold].to_dict(orient='records')\n",
    "        \n",
    "        # One-hot encoding\n",
    "        dv_fold = DictVectorizer(sparse=False)\n",
    "        X_fold_train = dv_fold.fit_transform(fold_train_dicts)\n",
    "        X_fold_val = dv_fold.transform(fold_val_dicts)\n",
    "        \n",
    "        # Train model with current C\n",
    "        model_c = LogisticRegression(solver='liblinear', C=c, max_iter=1000)\n",
    "        model_c.fit(X_fold_train, y_fold_train)\n",
    "        \n",
    "        # Predict and calculate AUC\n",
    "        y_fold_pred_proba = model_c.predict_proba(X_fold_val)[:, 1]\n",
    "        auc_fold = roc_auc_score(y_fold_val, y_fold_pred_proba)\n",
    "        \n",
    "        auc_scores_c.append(auc_fold)\n",
    "        print(f\"  Fold {fold}: AUC = {auc_fold:.3f}\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean_auc_c = np.mean(auc_scores_c)\n",
    "    std_auc_c = np.std(auc_scores_c)\n",
    "    \n",
    "    results[c] = {\n",
    "        'mean': round(mean_auc_c, 3),\n",
    "        'std': round(std_auc_c, 3),\n",
    "        'scores': auc_scores_c\n",
    "    }\n",
    "    \n",
    "    print(f\"  Mean AUC: {round(mean_auc_c, 3)}\")\n",
    "    print(f\"  Std AUC: {round(std_auc_c, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Summary of Hyperparameter Tuning\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for c in c_values:\n",
    "    print(f\"C = {c:>10}: mean = {results[c]['mean']:.3f}, std = {results[c]['std']:.3f}\")\n",
    "\n",
    "# Find best C (highest mean, or lowest std if tied, or smallest C if still tied)\n",
    "max_mean = max(results[c]['mean'] for c in c_values)\n",
    "best_candidates = [c for c in c_values if results[c]['mean'] == max_mean]\n",
    "\n",
    "if len(best_candidates) > 1:\n",
    "    # If tied, select the one with lowest std\n",
    "    min_std = min(results[c]['std'] for c in best_candidates)\n",
    "    best_candidates = [c for c in best_candidates if results[c]['std'] == min_std]\n",
    "    \n",
    "    if len(best_candidates) > 1:\n",
    "        # If still tied, select smallest C\n",
    "        best_c = min(best_candidates)\n",
    "    else:\n",
    "        best_c = best_candidates[0]\n",
    "else:\n",
    "    best_c = best_candidates[0]\n",
    "\n",
    "print(f\"\\n✓ Answer: Best C value = {best_c}\")\n",
    "print(f\"  Mean AUC: {results[best_c]['mean']}\")\n",
    "print(f\"  Std AUC: {results[best_c]['std']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
